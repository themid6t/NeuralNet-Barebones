{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6e25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed40d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv(\"Datasets/digit-recognizer/train.csv\"))\n",
    "x_data, y_data = data[:, 1:data.shape[0]], data[:, 0:1]\n",
    "\n",
    "indices = np.arange(len(x_data))\n",
    "np.random.shuffle(indices)\n",
    "x_data, y_data = x_data[indices], y_data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b01bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.9\n",
    "idx = int(x_data.shape[0] * split)\n",
    "trainx, trainy = x_data[:idx]/255., y_data[:idx]\n",
    "x_validate, y_validate = x_data[idx:]/255., y_data[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ea1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list(tuple())) -> None:\n",
    "        \"\"\" Input as: \n",
    "            NeuralNetwork(\n",
    "                [\n",
    "                    ({no of features}, activation='{function}'), ---> input layer\n",
    "                    ({neurons/nodes}, activation='{function}')\n",
    "                    .\n",
    "                    .\n",
    "                    .\n",
    "                    ({neurons/nodes}, activation='{function}') -----> output layer\n",
    "                ]\n",
    "            )\"\"\"\n",
    "        np.random.seed(1024)\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.activationFunc = [a for _, (_, a) in enumerate(self.layers)]\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.params = {}\n",
    "        \n",
    "        for l in range(1, self.num_layers):\n",
    "            self.params[f'w{l}'] = np.random.rand(self.layers[l][0], self.layers[l-1][0]) - 0.5\n",
    "            self.params[f'b{l}'] = np.random.rand(self.layers[l][0], 1) - 0.5\n",
    "    \n",
    "    def _activation(self, func: str, z, backprop=False):\n",
    "        if func == 'relu':\n",
    "            if backprop:\n",
    "                return z > 0\n",
    "            return np.maximum(0, z)\n",
    "        \n",
    "        elif func == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        elif func == 'softmax':\n",
    "            exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "            return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    def _loss(self, func, y_predicted, labels):\n",
    "        labels = OneHotEncoder().fit_transform(labels).toarray().T\n",
    "        if func == 'cross_entropy':\n",
    "            _, n = labels.shape\n",
    "            epsilon = 1e-15\n",
    "            y_predicted = np.maximum(epsilon, y_predicted)\n",
    "            loss = -1/n * np.sum(labels * np.log(y_predicted))\n",
    "            return loss\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        z_a = {}\n",
    "        for l in range(self.num_layers-1):\n",
    "            if l == 0:\n",
    "                z_a[f\"z{l+1}\"] = self.params[f\"w{l+1}\"].dot(x) + self.params[f\"b{l+1}\"]\n",
    "                z_a[f\"a{l+1}\"] = self._activation(self.activationFunc[l+1], z_a[f\"z{l+1}\"])\n",
    "            else:\n",
    "                z_a[f\"z{l+1}\"] = self.params[f\"w{l+1}\"].dot(z_a[f\"a{l}\"]) + self.params[f\"b{l+1}\"]\n",
    "                z_a[f\"a{l+1}\"] = self._activation(self.activationFunc[l+1], z_a[f\"z{l+1}\"])\n",
    "        \n",
    "        return z_a\n",
    "    \n",
    "    def _back_prop(self, zanda, x, y):\n",
    "        m, _ = x.shape\n",
    "        gradients = {}\n",
    "        dz = {}\n",
    "        dz_prev = None\n",
    "        for l in reversed(range(1, self.num_layers)):\n",
    "            if l == self.num_layers - 1:\n",
    "                dA = zanda[f\"a{self.num_layers-1}\"] - y\n",
    "                dZ = dA\n",
    "                gradients[f\"dw{l}\"] = 1 / m * dZ.dot(zanda[f\"a{l-1}\"].T)\n",
    "                gradients[f\"db{l}\"] = 1 / m * np.sum(dZ)\n",
    "                dz_prev = dZ\n",
    "            \n",
    "            elif l > 1:\n",
    "                w = self.params[f\"w{l+1}\"]\n",
    "                dA = w.T.dot(dz_prev)\n",
    "                dZ = dA * self._activation(self.activationFunc[l], zanda[f\"z{l}\"], backprop=True)\n",
    "                gradients[f\"dw{l}\"] = 1 / m * dZ.dot(zanda[f\"z{l-1}\"].T)\n",
    "                gradients[f\"db{l}\"] = 1 / m * np.sum(dZ)\n",
    "                dz_prev = dZ\n",
    "            \n",
    "            elif l == 1:\n",
    "                dA = self.params[f\"w{l+1}\"].T.dot(dz_prev)\n",
    "                dZ = dA * self._activation(self.activationFunc[l], zanda[f\"z{l}\"], backprop=True)\n",
    "                gradients[f\"dw{l}\"] = 1 / m * dZ.dot(x.T)\n",
    "                gradients[f\"db{l}\"] = 1 / m * np.sum(dZ)\n",
    "\n",
    "        return gradients\n",
    "    \n",
    "    \n",
    "    def fit(self, x_train, y_train, learning_rate, lossFunction, validation_data=None, epochs=10, batch_size=32):\n",
    "        \"\"\" Train the neural network based on training data.\n",
    "            Input as: model.fit(\n",
    "                            x_train, y_train, \n",
    "                            epoch=n, \n",
    "                            learning_rate=alpha, \n",
    "                            validation_data=(x, y), \n",
    "                            batch_size=32\n",
    "                        )\"\"\"\n",
    "        m, _ = x_train.shape\n",
    "        y_train_encoded = OneHotEncoder().fit_transform(y_train).toarray()\n",
    "        sub_steps = m // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # --------------------------------------\n",
    "            start_idx, end_idx = 0, batch_size\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            indices = np.arange(len(x_train))\n",
    "            np.random.shuffle(indices)\n",
    "            x_train, y_train_encoded = x_train[indices], y_train_encoded[indices]\n",
    "            y_train = y_train[indices]\n",
    "            # -------------------------------------\n",
    "            total_loss = 0\n",
    "            total_samples = 0\n",
    "            correct_predictions = 0\n",
    "            for step in range(sub_steps+1):\n",
    "                print(f\"{step+1}/{sub_steps}\", end=\"\\r\")\n",
    "                # ------- transformations --------------------------------\n",
    "                x, y = x_train[start_idx:end_idx].T, y_train_encoded[start_idx:end_idx].T\n",
    "                yacc = y_train[start_idx:end_idx].T\n",
    "                start_idx, end_idx = end_idx, end_idx+batch_size\n",
    "                # --------------------------------------------------------\n",
    "                # -------- training --------------------------------------\n",
    "                za = self._forward_prop(x)\n",
    "                gradients = self._back_prop(za, x, y)\n",
    "                for params in self.params.keys():\n",
    "                    self.params[params] -= learning_rate * gradients[f\"d{params}\"]\n",
    "                # --------------------------------------------------------\n",
    "                # ----------- loss and acc ------------------------------\n",
    "            ## Calculating accuracy using validation data\n",
    "            print(f\"{sub_steps}/{sub_steps}\", end=\" \")\n",
    "            validate = self._forward_prop(x_validate.T)\n",
    "            accuracy = self.accuracy(validate[f\"a{self.num_layers-1}\"], y_validate.T)\n",
    "            loss = self._loss(lossFunction, validate[f\"a{self.num_layers-1}\"], y_validate)\n",
    "            print(f\"val_acc: {accuracy} - val_loss: {loss}\")\n",
    "            \n",
    "    def accuracy(self, y_pred, y):\n",
    "        predictions = self.get_predictions(y_pred)\n",
    "        acc = np.sum(predictions == y) / y.size\n",
    "        return acc\n",
    "        \n",
    "    def get_predictions(self, y_pred):\n",
    "        return np.argmax(y_pred, axis=0)\n",
    "\n",
    "    def summary(self):\n",
    "        for i, (nodes, activation) in enumerate(self.layers):\n",
    "            print(f\"Layer {i}:\\n\\tNodes: {nodes} \\n\\tActivation: {activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a92cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    [\n",
    "        (784, 'relu'),\n",
    "        (256, 'relu'),\n",
    "#         (128, 'relu'),\n",
    "        (64, 'relu'),\n",
    "        (10, 'softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526dca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "1181/1181 val_acc: 0.7983333333333333 - val_loss: 0.7487279260091708\n",
      "\n",
      "Epoch 2/5\n",
      "1181/1181 val_acc: 0.840952380952381 - val_loss: 0.5638374991908717\n",
      "\n",
      "Epoch 3/5\n",
      "1181/1181 val_acc: 0.8607142857142858 - val_loss: 0.48600685112653746\n",
      "\n",
      "Epoch 4/5\n",
      "1181/1181 val_acc: 0.875 - val_loss: 0.44243270807327234\n",
      "\n",
      "Epoch 5/5\n",
      "1181/1181 val_acc: 0.878095238095238 - val_loss: 0.41822121867097467\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    trainx, trainy, \n",
    "    epochs=5, \n",
    "    learning_rate=0.1,  \n",
    "    lossFunction='cross_entropy',\n",
    "    batch_size=32,\n",
    "    validation_data = (x_validate, y_validate)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN-scratch",
   "language": "python",
   "name": "nn-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
