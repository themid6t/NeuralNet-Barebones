{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cde70a9",
   "metadata": {},
   "source": [
    "## Neural Network completely from scratch\n",
    "Implementation of a simple 4 layer neural network, and trained it on the MNIST digit recogniszer dataset.\n",
    "I am going to use only numpy for calculations and the OneHotEncoder from sklearn just to One Hot Encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd51a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccb56c",
   "metadata": {},
   "source": [
    "Load the dataset and split it into 90 and 10 percent.\n",
    "90% for training and 10% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46f5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv(\"Datasets/digit-recognizer/train.csv\"))\n",
    "x_data, y_data = data[:, 1:data.shape[0]], data[:, 0:1]\n",
    "indices = np.arange(len(x_data))\n",
    "np.random.shuffle(indices)\n",
    "x_data, y_data = x_data[indices], y_data[indices]\n",
    "\n",
    "split = 0.9\n",
    "idx = int(x_data.shape[0] * split)\n",
    "trainx, trainy = x_data[:idx]/255., y_data[:idx]\n",
    "x_validate, y_validate = x_data[idx:]/255., y_data[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f593dc",
   "metadata": {},
   "source": [
    "## Step 1: Neural Network Architecture\n",
    "The architecture will be: <br>\n",
    "    <pre>Input layer: 784 neurons \n",
    "    Hidden layer 1: 16 neurons \n",
    "    Hidden layer 2: 16 neurons \n",
    "    Output layer: 10 neurons </pre>\n",
    "So we will have 3 pairs of parameters i.e. <br>\n",
    "Since we have 4 layers including input and output layer, we will have 3 pairs of parameters (weights and biases):\n",
    "- **w1** *(16, 784)* and **b1** *(16, 1)* representing the weights and biases of the connections between the input layer *(784 neuron)* and the first hidden layer *(16 neuron).*\n",
    "- **w2** *(16, 16)* and **b1** *(16, 1)* representing the weights and biases of the connections between the hidden layer 1 *(16 neuron)* and the hidden layer 2 *(16 neuron).*\n",
    "- **w3** *(10, 16)* and **b1** *(10, 1)* representing the weights and biases of the connections between the hidden layer 2 *(16 neuron)* and the output layer *(10 neuron).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4114b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    w1 = np.random.rand(16, 784) - 0.5\n",
    "    b1 = np.random.rand(16, 1) - 0.5\n",
    "    \n",
    "    w2 = np.random.rand(16, 16) - 0.5\n",
    "    b2 = np.random.rand(16, 1) - 0.5\n",
    "    \n",
    "    w3 = np.random.rand(10, 16) - 0.5\n",
    "    b3 = np.random.rand(10, 1) - 0.5\n",
    "    \n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a85b6",
   "metadata": {},
   "source": [
    "## Step 2: Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647da8d9",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "> Lets first define the activation functions, the activation function will simply filter out and activate only those neurons which will make a significant difference in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e22570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(func: str, z, backprop=False):\n",
    "    if func == 'relu':\n",
    "        if backprop:\n",
    "            return z > 0\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    elif func == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    elif func == 'softmax':\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead85c1",
   "metadata": {},
   "source": [
    "### The main feed forward part\n",
    "In forward propagation we take all of the activations from the previous layer and and computer their weighted sum according to their weight and then move on to the next layer to perform the same.\n",
    "\n",
    ">  - From the input layer to first hidden layer\n",
    "    $$Z_{1} = W_{1} \\cdot X + b_{1}$$\n",
    "    $$A_{1} = g_{\\text{ ReLU}}(Z_{1}))$$\n",
    ">  - First hidden layer to second hidden layer\n",
    "    $$Z_{2} = W_{2} \\cdot A_{1} + b_{2}$$\n",
    "    $$A_{2} = g_{\\text{ ReLU}}(Z_{2})$$\n",
    ">  - Second hidden layer to Output layer\n",
    "    $$Z_{3} = W_{3} \\cdot A_{2} + b_{3}$$\n",
    "    $$A_{3} = g_{\\text{ softmax}}(Z_{3})$$\n",
    "\n",
    "The output layers final activation *A_{3}* are the predictions for the given input, these predictions are in the form of probability ranging from (0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53397058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(param, x):\n",
    "    w1, b1, w2, b2, w3, b3 = param\n",
    "    # input layer to hidden layer 1\n",
    "    z1 = w1.dot(x) + b1\n",
    "    a1 = activation('relu', z1)\n",
    "    # hidden layer 1 to hidden layer 2\n",
    "    z2 = w2.dot(a1) + b2\n",
    "    a2 = activation('relu', z2)\n",
    "    # hidden layer 2 to output layer\n",
    "    z3 = w3.dot(a2) + b3\n",
    "    a3 = activation('softmax', z3)\n",
    "    \n",
    "    return z1, a1, z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e6d9d",
   "metadata": {},
   "source": [
    "## Step 3: Loss / Error Calculation\n",
    "Now we have a method to get prediction based on given inputs, now we need to measure how **wrong** our predictions are, we use a loss function to measure that. Basically we measure how much the predictions are deviating from the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5073ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(func: str, y_predicted, labels):\n",
    "    if func == 'cross_entropy':\n",
    "        _, n = labels.shape\n",
    "        epsilon = 1e-15\n",
    "        y_predicted = np.maximum(epsilon, y_predicted)\n",
    "        loss = -1/n * np.sum(labels * np.log(y_predicted))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b39f9",
   "metadata": {},
   "source": [
    "## Step 4: Learning Algorithm, Backpropagation\n",
    "The final part of the neural network is to make it learn. For that we need to backpropagate the network and calculate the gradients, i.e. we calculate the gradients of output layer to the previous hidden layer, then of the hidden layer to the previous hidden layer ....\n",
    "1. **Output layer (`W3`, `b3`) gradients:**\n",
    "   $$dz3 = a3 - y$$\n",
    "   $$dW3 = \\frac{1}{m} \\cdot dz3 \\cdot a2^T$$\n",
    "   $$db3 = \\frac{1}{m} \\cdot \\text{sum}(dz3)$$\n",
    "2. **Hidden layer 2 (`W2`, `b2`) gradients:**\n",
    "   $$dz2 = W3^T \\cdot dz3 \\cdot \\text{ReLU'}(z2)$$\n",
    "   $$dW2 = \\frac{1}{m} \\cdot dz2 \\cdot z1^T$$\n",
    "   $$db2 = \\frac{1}{m} \\cdot \\text{sum}(dz2)$$\n",
    "3. **Hidden layer 1 (`W1`, `b1`) gradients:**\n",
    "   $$dz1 = W2^T \\cdot dz2 \\cdot \\text{ReLU'}(z1)$$\n",
    "   $$dW1 = \\frac{1}{m} \\cdot dz1 \\cdot x^T$$\n",
    "   $$db1 = \\frac{1}{m} \\cdot \\text{sum}(dz1)$$\n",
    "**So finally we have the gradients `(dw1, db1), (dw2, db2), (dw3, db3)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad0b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(zanda, params, x, y):\n",
    "    m, _ = x.shape\n",
    "    z1, a1, z2, a2, z3, a3 = zanda\n",
    "    w1, b1, w2, b2, w3, b3 = params\n",
    "    \n",
    "    # Output layer gradients\n",
    "    dz3 = a3 - y \n",
    "    dw3 = 1 / m * dz3.dot(a2.T)\n",
    "    db3 = 1 / m * np.sum(dz3)\n",
    "    # Hidden layer 2 gradient\n",
    "    dz2 = w3.T.dot(dz3) * activation('relu', z2, backprop=True)\n",
    "    dw2 = 1 / m * dz2.dot(z1.T)\n",
    "    db2 = 1 / m * np.sum(dz2)\n",
    "    # Hidden layer 1 gradient\n",
    "    dz1 = w2.T.dot(dz2) * activation('relu', z1, backprop=True)\n",
    "    dw1 = 1 / m * dz1.dot(x.T)\n",
    "    db1 = 1 / m * np.sum(dz1)\n",
    "    \n",
    "    return dw1, db1, dw2, db2, dw3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476b86e",
   "metadata": {},
   "source": [
    "### Adjust the parameters to minimize the loss of the network\n",
    "We have the gradients of each layer `(dw1, db1), (dw2, db2), (dw3, db3)`\n",
    "Now we update the parameter by moving them closer to the global minima. We do it in small steps, that will be determined by some learning rate $\\alpha$.\n",
    "**Peform Gradient Descent**\n",
    "1. For the output layer (W3, b3):\n",
    "   $$W_3 -= \\alpha \\times dW_3$$\n",
    "   $$b_3 -= \\alpha \\times db_3$$\n",
    "\n",
    "2. For hidden layer 2 (W2, b2):\n",
    "   $$W_2 -= \\alpha \\times dW_2$$\n",
    "   $$b_2 -= \\alpha \\times db_2$$\n",
    "\n",
    "3. For hidden layer 1 (W1, b1):\n",
    "   $$W_1 -= \\alpha \\times dW_1$$\n",
    "   $$b_1 -= \\alpha \\times db_1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180760f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(param, gradient, learning_rate):\n",
    "    w1, b1, w2, b2, w3, b3 = param\n",
    "    dw1, db1, dw2, db2, dw3, db3 = gradient\n",
    "    \n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "    w3 -= learning_rate * dw3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb7d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "        predictions = np.argmax(y_pred, axis=0)\n",
    "        acc = np.sum(predictions == y) / y.size\n",
    "#         print(f\"pp {predictions.shape}, {y.shape}, {y.size}\")\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747060f",
   "metadata": {},
   "source": [
    "**We need to one hot encoded the lables for the loss calculation and gradients calculation, so we use the OneHotEncoder from Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc42ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_y = OneHotEncoder().fit_transform(trainy).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc77bc",
   "metadata": {},
   "source": [
    "##### **Now we use all of the methods to create the network and optimize the parameters, We train the network for 1000 epoch i.e. 1000 iteration on the given dataset.**\n",
    "**And calculate and observe the loss and acuraccy for every 100 steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943682ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000\n",
      "loss: 3.67429459149603 - acc: 0.09526455026455026\n",
      "Epoch 100/1000\n",
      "loss: 0.5646364463315656 - acc: 0.8224867724867725\n",
      "Epoch 200/1000\n",
      "loss: 0.40460060787897173 - acc: 0.8705291005291005\n",
      "Epoch 300/1000\n",
      "loss: 0.34739063712420404 - acc: 0.8933333333333333\n",
      "Epoch 400/1000\n",
      "loss: 0.29898336558231814 - acc: 0.9082804232804232\n",
      "Epoch 500/1000\n",
      "loss: 0.2694288843430095 - acc: 0.9176455026455026\n",
      "Epoch 600/1000\n",
      "loss: 0.25413424900479853 - acc: 0.9219312169312169\n",
      "Epoch 700/1000\n",
      "loss: 0.24328867098892756 - acc: 0.9257671957671958\n",
      "Epoch 800/1000\n",
      "loss: 0.22327413517104425 - acc: 0.9316931216931217\n",
      "Epoch 900/1000\n",
      "loss: 0.2124299192578787 - acc: 0.9352380952380952\n",
      "Epoch 1000/1000\n",
      "loss: 0.22561748411273364 - acc: 0.9302910052910053\n"
     ]
    }
   ],
   "source": [
    "params = init_params()\n",
    "epoch = 1000\n",
    "learning_rate = 0.01\n",
    "for i in range(epoch+1):\n",
    "    zanda = forward_prop(params, trainx.T)\n",
    "    gradients = back_prop(zanda, params, trainx.T, one_hot_y.T)\n",
    "    params = gradient_descent(params, gradients, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i}/{epoch}\")\n",
    "        mloss = loss('cross_entropy', zanda[-1], one_hot_y.T)\n",
    "        acc = accuracy(zanda[-1], trainy.T)\n",
    "        print(f\"loss: {mloss} - acc: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391d220",
   "metadata": {},
   "source": [
    "#### **Our neural network is giving 93% accuracy with low loss which is fair for something built from scratch now lets validate it by testing on some unseen data.**\n",
    "**For testing we just need to do a forward pass through the network on the unseen data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af2729ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(x_val, y_val):\n",
    "    enc_y = OneHotEncoder().fit_transform(y_val).toarray()\n",
    "    _, _, _, _, _, a3 = forward_prop(params, x_val.T)\n",
    "    vloss = loss('cross_entropy', a3, enc_y.T)\n",
    "    acc = accuracy(a3, y_val.T)\n",
    "    print(f\"val_loss: {vloss} - val_acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df53df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.23088474886439597 - val_acc: 0.9369047619047619\n"
     ]
    }
   ],
   "source": [
    "validate(x_validate, y_validate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN-scratch",
   "language": "python",
   "name": "nn-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
